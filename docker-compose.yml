version: "3.7"
services:
    postgres:
        container_name: postgres
        image: postgres:9.6
        environment:
            - POSTGRES_USER=${AIRFLOW}
            - POSTGRES_PASSWORD=${AIRFLOW}
            - POSTGRES_DB=${AIRFLOW}
        logging:
            options:
                max-size: 10m
                max-file: "3"        

    airflow:
        container_name: airflow
        build: ./airflow
        restart: unless-stopped
        logging:
            options:
                max-size: 10m
                max-file: "3"
        command: ["webserver"]
        ports:
            - 8080:8080
        volumes:
            - ./airflow/dags:/usr/local/airflow/dags
            - ./airflow/modules:/usr/local/airflow/modules
        healthcheck:
            test: ["CMD-SHELL", "[ -f /usr/local/airflow/airflow-webserver.pid ]"]
            interval: 30s
            timeout: 30s
            retries: 3
        depends_on:
            - postgres

    zookeeper:
        container_name: zookeeper
        image: confluentinc/cp-zookeeper:7.6.0
        environment:
            - ZOOKEEPER_CLIENT_PORT=${ZOOKEEPER_CLIENT_PORT}
            - ZOOKEEPER_TICK_TIME=${ZOOKEEPER_TICK_TIME}
            - ZOOKEEPER_JUTE_MAXBUFFER=${ZOOKEEPER_JUTE_MAXBUFFER}
        ports:
            - 2181:2181

    kafka:
        container_name: kafka
        build: ./kafka
        environment: 
            - DEFAULT_TOPIC=${DEFAULT_TOPIC}
            - KAFKA_ZOOKEEPER_CONNECT=${ZOOKEEPER_URL}
            - KAFKA_ADVERTISED_LISTENERS=LISTENER_DOCKER_INTERNAL://kafka:19092,LISTENER_DOCKER_EXTERNAL://${DOCKER_HOST_IP:-127.0.0.1}:9092
            - KAFKA_LISTENER_SECURITY_PROTOCOL_MAP=LISTENER_DOCKER_INTERNAL:PLAINTEXT,LISTENER_DOCKER_EXTERNAL:PLAINTEXT
            - KAFKA_INTER_BROKER_LISTENER_NAME=LISTENER_DOCKER_INTERNAL
            - KAFKA_LOG4J_ROOT_LOGLEVEL=ERROR
            - KAFKA_CONFLUENT_SUPPORT_METRICS_ENABLE="false"
            - KAFKA_ALLOW_EVERYONE_IF_NO_ACL_FOUND="true"
            - KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR= 1
            - KAFKA_LISTENERS=LISTENER_DOCKER_INTERNAL://0.0.0.0:19092,LISTENER_DOCKER_EXTERNAL://0.0.0.0:9092
#            - KAFKA_LISTENERS=PLAINTEXT://kafka:9092
#            - ZOOKEEPER_URL=zookeeper:2181
#            - KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://kafka:9092
#            - KAFKA_LISTENER_SECURITY_PROTOCOL_MAP=PLAINTEXT


        depends_on:
            - zookeeper
        ports:
            - 9092:9092

    schema-registry:
        container_name: schema-registry
        image: confluentinc/cp-schema-registry:7.6.0
        environment:
            - SCHEMA_REGISTRY_HOST_NAME=${SCHEMA_REGISTRY_HOST_NAME}
#            - SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL=${ZOOKEEPER_URL}
            - SCHEMA_REGISTRY_LOG4J_ROOT_LOGLEVEL=${SCHEMA_REGISTRY_LOGLEVEL}
            - SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS=PLAINTEXT://kafka:19092
            - SCHEMA_REGISTRY_LISTENERS=http://0.0.0.0:8081
            - SCHEMA_REGISTRY_DEBUG=true
        ports:
            - "8081:8081"
        depends_on:
            - kafka

    redis:
        container_name: redis
        image: redis:alpine
        command: ["redis-server"]
        ports:
            - 6379:6379
        volumes:
            - ./redis:/usr/local/etc/redis

#    connect:
#        container_name: connect
#        build: ./connect
#        environment:
#            - CONNECT_REST_ADVERTISED_HOST_NAME=${CONNECT_REST_ADVERTISED_HOST_NAME}
#            - CONNECT_SCHEMA_REGISTRY_URL=${SCHEMA_REGISTRY_URL}
#            - CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL=${SCHEMA_REGISTRY_URL}
#            - CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL=${SCHEMA_REGISTRY_URL}
#            - CONNECT_BOOTSTRAP_SERVERS=${CONNECT_BOOTSTRAP_SERVERS}
#            - CONNECT_BOOTSTRAP_SERVERS=kafka:19092
#            - CONNECT_REST_PORT=8083
#            - CONNECT_GROUP_ID="connect-cluster"
#            - CONNECT_CONFIG_STORAGE_TOPIC="connect-config"
#            - CONNECT_OFFSET_STORAGE_TOPIC="connect-offsets"
#            - CONNECT_STATUS_STORAGE_TOPIC="connect-status"
#            - CONNECT_KEY_CONVERTER="org.apache.kafka.connect.storage.StringConverter"
#            - CONNECT_VALUE_CONVERTER="org.apache.kafka.connect.storage.StringConverter"
#            - CONNECT_INTERNAL_KEY_CONVERTER="org.apache.kafka.connect.json.JsonConverter"
#            - CONNECT_INTERNAL_VALUE_CONVERTER="org.apache.kafka.connect.json.JsonConverter"
#            - CONNECT_REST_ADVERTISED_HOST_NAME=connect
#            - CONNECT_LOG4J_ROOT_LOGLEVEL=INFO
#            - CONNECT_PLUGIN_PATH=/usr/share/java
#        ports:
#            - 8083:8083
#        depends_on:
#            - kafka
#            - schema-registry
#            - api
#            - minio

    connect:
        image: connect
        build: /connect
        depends_on:
            - kafka
            - schema-registry
            - api
            - minio
        ports:
            - "8083:8083"
        environment:
            CONNECT_BOOTSTRAP_SERVERS: 'kafka:19092'
            CONNECT_REST_ADVERTISED_HOST_NAME: connect
            CONNECT_GROUP_ID: compose-connect-group
            CONNECT_CONFIG_STORAGE_TOPIC: docker-connect-configs
            CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1
            CONNECT_OFFSET_FLUSH_INTERVAL_MS: 10000
            CONNECT_OFFSET_STORAGE_TOPIC: docker-connect-offsets
            CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1
            CONNECT_STATUS_STORAGE_TOPIC: docker-connect-status
            CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1
            CONNECT_KEY_CONVERTER: org.apache.kafka.connect.storage.StringConverter
            CONNECT_VALUE_CONVERTER: io.confluent.connect.avro.AvroConverter
            CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: http://schema-registry:8081
            # CLASSPATH required due to CC-2422
            CLASSPATH: /usr/share/java/monitoring-interceptors/monitoring-interceptors-7.6.0.jar
            CONNECT_PRODUCER_INTERCEPTOR_CLASSES: "io.confluent.monitoring.clients.interceptor.MonitoringProducerInterceptor"
            CONNECT_CONSUMER_INTERCEPTOR_CLASSES: "io.confluent.monitoring.clients.interceptor.MonitoringConsumerInterceptor"
            CONNECT_PLUGIN_PATH: "/usr/share/java,/usr/share/confluent-hub-components"
            CONNECT_LOG4J_LOGGERS: org.apache.zookeeper=ERROR,org.I0Itec.zkclient=ERROR,org.reflections=ERROR

    elasticsearch:
        container_name: elasticsearch
        build: ./elastic
        restart: unless-stopped
        ulimits:
            memlock:
                soft: -1
                hard: -1
        ports:
            - 9200:9200

    mongo:
        container_name: mongo
        image: mongo:4.2
        command: ["mongod", "--replSet", "rs0", "--auth"]
        environment: 
            - MONGO_ADMIN=${MONGO_ADMIN}
            - RSS_NEWS_USER=${RSS_NEWS}
            - MONGO_INITDB_ROOT_USERNAME=${RSS_NEWS}
            - MONGO_INITDB_ROOT_PASSWORD=${RSS_NEWS}
        ports:
            - 27017:27017
        volumes:
            - ./mongo/init.sh:/usr/local/bin/init.sh
        
    api:
        container_name: api
        build: ./api
        restart: unless-stopped
        environment:
            - ELASTIC_HOST=${ELASTIC_HOST}
            - ELASTIC_INDEX=${ELASTIC_INDEX}
            - MONGO_HOST=${MONGO_HOST}
            - MONGO_USR=${RSS_NEWS}
            - MONGO_PASSWD=${RSS_NEWS}
            - ALLOWED_HOSTS=${ALLOWED_HOSTS}
            - MONGO_DB_NAME=${RSS_NEWS}
        volumes:
            - ./api/app:/app
            - static_data:/app/static
        depends_on:
            - mongo
            - elasticsearch
    
    proxy:
        container_name: proxy
        build: ./api/proxy
        ports: 
            - 5000:5000
        volumes: 
            - static_data:/vol/static 
        depends_on: 
            - api

    minio:
        container_name: minio
        build: ./minio
        ports:
            - 9000:9000
        environment:
            - MINIO_ACCESS_KEY=${MINIO_ACCESS_KEY}
            - MINIO_SECRET_KEY=${MINIO_SECRET_KEY}
            - DEFAULT_BUCKET=${MINIO_DEFAULT_BUCKET}

networks: 
    default:
        name: backend-net

volumes:
    static_data: {}